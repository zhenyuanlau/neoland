= LiteLLM

link:https://www.litellm.ai/[LiteLLM] handles loadbalancing, fallbacks and spend tracking across 100+ LLMs. All in the OpenAI format.

[excalidraw]

----
include::partial$llm.excalidraw[]
----

== OpenAI Proxy

[source, shell]

----
pip install 'litellm[proxy]'

litellm --model ollama_chat/llama3
----

== LiteLLM UI

[source, shell]
------
make litellm.ui
------
