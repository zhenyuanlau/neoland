<!-- livebook:{"persist_outputs":true} -->

# Let's build GPT from scratch! w/ Nx and Axon

```elixir
Mix.install(
  [
    {:nx, "~> 0.5.3"},
    {:req, "~> 0.3.6"},
    {:kino_bumblebee, "~> 0.3.0"},
    {:exla, "~> 0.5.1"},
    {:table_rex, "~> 3.1.1"}
  ],
  config: [nx: [default_backend: EXLA.Backend]]
)
```

## Introduction

This notebook covers Andrej Karpathy's video [Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY) We'll start off building a simple bigram model, and iteratively build up to the decoder-only transformer.

Note: this notebook was created to experiment with Elixir's ML libraries, so the following code is probably not idiomatic Nx/Axon code and doesn't take full advantage of their capabilities.

### References

* Karpathy's companion notebook can be found [here](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing#scrollTo=wJpXpmjEYC_T)

* Thanks to Lorenzo Sinisi for the initial livebook [code](https://gist.github.com/lorenzosinisi/bb928554d665bdc53aada98c3710b0d5)

## Prepare data

Let's first prepare our Shakespeare data

```elixir
file_path = Path.absname("./input.txt")

text =
  if File.exists?(file_path) do
    IO.puts("File loaded from memory: #{file_path}")
    File.read!(file_path)
  else
    IO.puts(
      "File loaded from git: https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
    )

    Req.get!(
      "https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
    ).body
  end
```

<!-- livebook:{"output":true} -->

```
File loaded from git: https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt
```

<!-- livebook:{"output":true} -->

```
"First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge.\n\nSecond Citizen:\nWould you proceed especially against Caius Marcius?\n\nAll:\nAgainst him first: he's a very dog to the commonalty.\n\nSecond Citizen:\nConsider you what services he has done for his country?\n\nFirst Citizen:\nVery well; and could be content to give him good\nreport fort, but that he pays himself with being proud.\n\nSecond Citizen:\nNay, but speak not maliciously.\n\nFirst Citizen:\nI say unto you, what he hath done famously, he did\nit to that end: though soft-conscienced men can be\ncontent to say it was for his country he did it to\nplease his mother and to be partly proud; which he\nis, even till the altitude of his virtue.\n\nSecond Citizen:\nWhat he cannot help in his nature, you account a\nvice in him. You must in no way say he is covetous.\n\nFirst Citizen:\nIf I must not, I need not be barren of accusations;\nhe hath faults, with surplus, to tire in repetition.\nWhat shouts are these? The other side o' the city\nis risen: why stay we prating here? to the Capitol!\n\nAll:\nCome, come.\n\nFirst Citizen:\nSoft! who comes here?\n\nSecond Citizen:\nWorthy Menenius Agrippa; one that hath always loved\nthe people.\n\nFirst Citizen:\nHe's one honest enough: would all the rest were so!\n\nMENENIUS:\nWhat work's, my countrymen, in hand? where go you\nWith bats and clubs? The matter? speak, I pray you.\n\nFirst Citizen:\nOur business is not unknown to the senate; they have\nhad inkling this fortnight what we intend to do,\nwhich now we'll show 'em in deeds. They say poor\nsuitors have strong breaths: they shall know we\nhave strong arms too.\n\nMENENIUS:\nWhy, masters, my good friends, mine honest neighbours,\nWill you undo yourselves?\n\nFirst Citizen:\nWe cannot, sir, we are undone already.\n\nMENENIUS:\nI tell you, friends, most charitable care\nHave the patricians of you. For your wants,\nYour suffering in this dearth, you may as well\nStrike at the heaven with your staves as lift them\nAgainst the Roman state, whose course will on\nThe way it takes, cracking ten thousand curbs\nOf more strong link asunder than can ever\nAppear in your impediment. For the dearth,\nThe gods, not the patricians, make it, and\nYour knees to them, not arms, must help. Alack,\nYou are transported by calamity\nThither where more attends you, and you slander\nThe helms o' the state, who care for you like fathers,\nWhen you curse them as enemies.\n\nFirst Citizen:\nCare for us! True, indeed! They ne'er cared for us\nyet: suffer us to famish, and their store-houses\ncrammed with grain; make edicts for usury, to\nsupport usurers; repeal daily any wholesome act\nestablished against the rich, and provide more\npiercing statutes daily, to chain up and restrain\nthe poor. If the wars eat us not up, they will; and\nthere's all the love they bear us.\n\nMENENIUS:\nEither you must\nConfess yourselves wondrous malicious,\nOr be accused of folly. I shall tell you\nA pretty tale: it may be you have heard it;\nBut, since it serves my purpose, I will venture\nTo stale 't a little more.\n\nFirst Citizen:\nWell, I'll hear it, sir: yet you must not think to\nfob off our disgrace with a tale: but, an 't please\nyou, deliver.\n\nMENENIUS:\nThere was a time when all " <> ...
```

## Basic Encoder / Decoder

```elixir
defmodule Minidecoder do
  @chars text |> String.codepoints() |> Enum.uniq() |> Enum.sort()
  @vocab_size Enum.count(@chars)
  def vocab_size, do: @vocab_size

  @stoi Enum.reduce(@chars, %{}, fn ch, acc -> Map.put(acc, ch, Enum.count(acc)) end)
  @itos Enum.reduce(@stoi, %{}, fn {ch, i}, acc -> Map.put(acc, i, ch) end)

  def encode_char(char), do: @stoi[char]

  def decode_char(encoded_char), do: @itos[encoded_char]

  def encode(text) do
    text |> String.codepoints() |> Enum.map(&encode_char(&1))
  end

  def decode(encoded_list) do
    encoded_list |> Enum.map(&decode_char(&1)) |> Enum.join()
  end

  def tensor(text) do
    Nx.tensor(encode(text))
  end
end

vocab_size =
  Minidecoder.vocab_size()
  |> IO.inspect(label: "vocab size is")

Minidecoder.tensor(text)
```

<!-- livebook:{"output":true} -->

```
vocab size is: 65

16:17:17.158 [info] TfrtCpuClient created.

```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  s64[1115394]
  EXLA.Backend<host:0, 0.3254234594.1811546133.171330>
  [18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56, 43, 1, 61, 43, 1, 54, 56, 53, 41, 43, 43, 42, 1, 39, 52, 63, 1, 44, 59, 56, 58, 46, 43, 56, 6, 1, 46, 43, 39, 56, ...]
>
```

## Encoded Training + Validation Data

```elixir
data = Minidecoder.tensor(text)
n = Kernel.round(Nx.size(data) * 0.9)
# take from index 0 till the end
train_data = Nx.slice(data, [0], [n])
# take from index 0 for size - n (to get all until end)
val_data = Nx.slice(data, [n], [Nx.size(data) - n])
{train_data, val_data}
```

<!-- livebook:{"output":true} -->

```
{#Nx.Tensor<
   s64[1003855]
   EXLA.Backend<host:0, 0.3254234594.1811546128.171212>
   [18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56, 43, 1, 61, 43, 1, 54, 56, 53, 41, 43, 43, 42, 1, 39, 52, 63, 1, 44, 59, 56, 58, 46, 43, 56, 6, 1, 46, 43, 39, ...]
 >,
 #Nx.Tensor<
   s64[111539]
   EXLA.Backend<host:0, 0.3254234594.1811546128.171214>
   [0, 0, 19, 30, 17, 25, 21, 27, 10, 0, 19, 53, 53, 42, 1, 51, 53, 56, 56, 53, 61, 6, 1, 52, 43, 47, 45, 46, 40, 53, 59, 56, 1, 14, 39, 54, 58, 47, 57, 58, 39, 8, 0, 0, 14, 13, 28, 32, ...]
 >}
```

## Training Data

To speed up training, we're going to batch our training data.
It'll look like this.

```
x = [
  ["h", "e", "l", "l", "o"],
  [" ", "w", "o", "r", "l"]
]

y = [
  ["e", "l", "l", "o", " "],
  ["w", "o", "r", "l", "d"]
]
```

We'll insert a linear layer between our x and y. After training, the model should learn these associations

* "h" -> "e"
* "e" -> "l"
* ..
* "w" -> "o"
* "o" -> "r"
* etc

<!-- livebook:{"break_markdown":true} -->

### Training Data Generator

The Axon training loop expects an Enumerable or Stream for its training data. We'll use `Stream.resource/3` to repeatedly generate random slices of our training data. Everytime we call it, it'll also keep track of a random key for the next generation. This ensures reproducible model outputs.

We'll experiment with different batch sizes and block sizes, so we'll wrap this Stream in a closure.

```elixir
seed = 1337

get_batch_stream = fn batch_size, block_size, split ->
  Stream.resource(
    # initialization function
    fn ->
      Nx.Random.key(seed)
    end,
    # generation function
    fn key ->
      data = if(split == :train, do: train_data, else: val_data)

      {ix, new_key} =
        Nx.Random.randint(key, 0, Nx.size(data) - block_size, shape: {batch_size}, type: :u32)

      ix = Nx.to_list(ix)

      x = Enum.map(ix, fn i -> Nx.slice(data, [i], [block_size]) end) |> Nx.stack()
      y = Enum.map(ix, fn i -> Nx.slice(data, [i + 1], [block_size]) end) |> Nx.stack()

      # Reshape yb {b, t}, to be a single vector
      # We do this to match the shape of y_true during training
      # https://hexdocs.pm/axon/Axon.Losses.html#categorical_cross_entropy/3
      {b, t} = Nx.shape(y)

      # or Nx.flatten
      flattened_y = Nx.reshape(y, {b * t})

      out_data = {x, flattened_y}

      {[out_data], new_key}
    end,
    # termination function
    fn _ -> :ok end
  )
end

train_batch_stream = get_batch_stream.(4, 8, :train)
train_batch_stream |> Enum.take(1)
```

<!-- livebook:{"output":true} -->

```
[
  {#Nx.Tensor<
     s64[4][8]
     EXLA.Backend<host:0, 0.3254234594.1811546128.171801>
     [
       [46, 47, 51, 57, 43, 50, 44, 1],
       [26, 19, 1, 30, 21, 15, 20, 13],
       [41, 43, 42, 1, 39, 1, 58, 56],
       [1, 42, 53, 1, 46, 43, 56, 43]
     ]
   >,
   #Nx.Tensor<
     s64[32]
     EXLA.Backend<host:0, 0.3254234594.1811546128.171816>
     [47, 51, 57, 43, 50, 44, 1, 40, 19, 1, 30, 21, 15, 20, 13, 30, 43, 42, 1, 39, 1, 58, 56, 39, 42, 53, 1, 46, 43, 56, 43, 6]
   >}
]
```

## Simple Bigram Model

Let's assume we have a well trained bigram model.

Given an input tensor of size `{1, 4}` the output might look something like this.

```
# batch_size = 1, block_size = 4
input = [[h, e, l, l]]

# batch_size = 1, block_size = 4, vocab_size = 65
output = [[[65], [65], [65], [65]]]
```

* Each index in these [65] sized tensors correspond to an encoded character from our Shakespeare vocab size
* The likelihood of an encoded character appearing next in a sequence is given by its value inside the [65] sized tensor.

To predict the next character in our sequence, we'll look at the last [65] sized tensor in our output. Right now the values are just some raw, non-normalized predictions for our 65 possible characters. We'll feed this tensor (called logits) into softmax to get a probability distribution that we can sample the next character from.

```elixir
# Hyperparameters
batch_size = 4
block_size = 8

bigram_model =
  Axon.input("sequence")
  |> Axon.embedding(65, 65)

Axon.Display.as_graph(bigram_model, Nx.template({batch_size, block_size}, :f32),
  direction: :top_down
)
```

<!-- livebook:{"output":true} -->

```mermaid
graph TD;
11[/"sequence (:input) {4, 8}"/];
12["embedding_0 (:embedding) {4, 8, 65}"];
11 --> 12;
```

## Training the bigram model

```elixir
# We'll use this for other models further along in the notebook
defmodule CommonTrain do
  import Nx.Defn

  defn custom_predict_fn(model_predict_fn, params, input) do
    %{prediction: preds} = out = model_predict_fn.(params, input)
    {b, t, c} = Nx.shape(preds)
    reshaped = Nx.reshape(preds, {b * t, c})
    %{out | prediction: reshaped}
  end

  def custom_loss_fn(y_true, y_pred) do
    Axon.Losses.categorical_cross_entropy(y_true, y_pred,
      from_logits: true,
      sparse: true,
      reduction: :mean
    )
  end
end

{init_fn, predict_fn} = Axon.build(bigram_model, mode: :train)
custom_predict_fn = &CommonTrain.custom_predict_fn(predict_fn, &1, &2)
custom_loss_fn = &CommonTrain.custom_loss_fn(&1, &2)
train_batch_stream = get_batch_stream.(4, 8, :train)

params =
  {init_fn, custom_predict_fn}
  |> Axon.Loop.trainer(custom_loss_fn, Axon.Optimizers.adamw())
  |> Axon.Loop.run(train_batch_stream, %{}, epochs: 1, iterations: 10000, compiler: EXLA)
```

<!-- livebook:{"output":true} -->

```

16:17:39.504 [debug] Forwarding options: [compiler: EXLA] to JIT compiler
Epoch: 0, Batch: 9950, loss: 2.8661370
```

<!-- livebook:{"output":true} -->

```
%{
  "embedding_0" => %{
    "kernel" => #Nx.Tensor<
      f32[65][65]
      EXLA.Backend<host:0, 0.3254234594.1816002575.231685>
      [
        [1.9532405138015747, -4.392496585845947, -4.390341281890869, -4.384500980377197, -4.3944478034973145, -0.946657657623291, -4.388766765594482, -3.953263759613037, -4.3834428787231445, -2.316786050796509, -4.388502597808838, -4.379186630249023, -4.379351615905762, 1.277715802192688, 0.43034467101097107, 0.24472467601299286, -0.18639983236789703, -0.6705255508422852, 0.2654992341995239, -0.1371765434741974, 0.4490107595920563, 0.8433833718299866, -1.1276862621307373, -0.3937842547893524, -0.033992741256952286, 0.35430219769477844, -0.024245813488960266, 0.012728894129395485, -0.12281199544668198, -1.0454705953598022, -0.5036211013793945, 0.4443173110485077, 1.4624419212341309, -1.0150002241134644, -1.3738644123077393, 0.9729534983634949, -4.3954386711120605, -0.23948293924331665, -4.38663387298584, -1.2121442556381226, -1.6288490295410156, -1.3537789583206177, -1.6692707538604736, -2.2649142742156982, -1.613582730293274, -1.9191597700119019, -1.214747428894043, -1.704034686088562, ...],
        ...
      ]
    >
  }
}
```

## Generating text with the bigram model, w/ argmax

Let's implement a naive way of generating text using `Nx.argmax`. Everytime we make a prediction, argmax will pick the highest probable character that our model thinks should be next.

```elixir
generate_fn = fn model, params, init_seq, max_new_tokens ->
  Enum.reduce(1..max_new_tokens, init_seq, fn _i, acc ->
    {_b, t} = Nx.shape(acc)

    # Cap the input sequence length from [t, block size]
    context_length = min(t, block_size)
    context_range = -context_length..-1
    context_slice = acc[[.., context_range]]

    # Predict next char
    preds = Axon.predict(model, params, context_slice)
    logits = preds[[.., -1, ..]]
    probs = Axon.Activations.softmax(logits)
    # {b, 1}
    batch_char = Nx.argmax(probs, axis: 1, keep_axis: true)

    Nx.concatenate([acc, batch_char], axis: -1)
  end)
end

# init_seq = Nx.broadcast(0, {1, 1})
init_seq = Nx.iota({1, 5})
max_new_tokens = 500

generate_fn.(bigram_model, params, init_seq, max_new_tokens)
# Convert our Nx.tensor to Elixir list
|> Nx.to_list()
# Decode the results
|> Enum.map(fn encoded_list -> Minidecoder.decode(encoded_list) end)
# Our input just 1 batch, so grab the first one
|> List.first()
|> IO.puts()
```

<!-- livebook:{"output":true} -->

```

 !$&cour the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the
```

<!-- livebook:{"output":true} -->

```
:ok
```

Why the repetition?

Lets look inside our linear layer. We should see something like this

* "t" is likely to produce "h"
* "h" is likely to produce "e"
* "e" is likely to produce " "
* " " is likely to produce "t"

```elixir
get_top_predictions = fn char, table, num_chars ->
  encoded_char = Minidecoder.encode_char(char)
  predictions = table[encoded_char]

  predictions
  |> Nx.to_list()
  |> Enum.with_index(fn element, index -> {index, element} end)
  |> Enum.map(fn {idx, logit} -> {Minidecoder.decode_char(idx), logit} end)
  |> Enum.sort(fn {_x_idx, x_res}, {_y_idx, y_res} -> x_res >= y_res end)
  |> Enum.take(num_chars)
end

table = params["embedding_0"]["kernel"]

[
  t: get_top_predictions.("t", table, 3),
  h: get_top_predictions.("h", table, 3),
  e: get_top_predictions.("e", table, 3),
  _: get_top_predictions.(" ", table, 3)
]
```

<!-- livebook:{"output":true} -->

```
[
  t: [{"h", 2.568699598312378}, {" ", 2.2036147117614746}, {"o", 1.0894719362258911}],
  h: [{"e", 2.453514814376831}, {"a", 1.7327364683151245}, {"i", 1.424493670463562}],
  e: [{" ", 2.5152766704559326}, {"r", 1.7054251432418823}, {"n", 1.181536316871643}],
  _: [{"t", 1.9793086051940918}, {"a", 1.380100131034851}, {"h", 1.279040813446045}]
]
```

## Multinomial

To avoid repetitive text, we want to randomly sample a character with our model prediction. We can do this with `Nx.Random.choice/4`. But, our model's output shape is `{b, t, vocab_size}`. Pytorch's `torch.multinomial` can work with batches, but (afaik) there's no equivalent function in the Nx library. We'll need to write a custom function to stack the results of `Nx.Random.choice/4`

```elixir
defmodule RandomPlus do
  import Nx.Defn

  defn multinomial(init_key, input, opts \\ []) do
    opts = keyword!(opts, num_samples: 1)
    num_samples = opts[:num_samples]

    {b, c} = Nx.shape(input)
    initial_tensor = Nx.broadcast(0, {b, num_samples})
    category_iota = Nx.iota({c}, type: :s32)

    {_i, _input, next_key, acc} =
      while {i = 0, input, key = init_key, acc = initial_tensor}, i < b do
        # Becomes {C}, represents probability distribution
        i_batch_prob = input[i]

        {i_samples, next_key} =
          Nx.Random.choice(key, category_iota, i_batch_prob, samples: num_samples)

        # Update ith row in acc to hold the new samples
        i_samples_reformatted = Nx.reshape(i_samples, {1, :auto})
        acc = Nx.put_slice(acc, [i, i], i_samples_reformatted)
        {i + 1, input, next_key, acc}
      end

    {next_key, acc}
  end
end

probs =
  Nx.tensor([
    [0.2, 0.3, 0.1, 0.15, 0.25],
    [0.10, 0.10, 0.10, 0.10, 0.60],
    [0.0, 0.0, 0.0, 0.0, 1.00]
  ])

# Given some batched probability distribution, sample 5 values
# This is for demonstration purposes (we'll only need to sample 1 char when generating text)
{_key, samples} = RandomPlus.multinomial(Nx.Random.key(1337), probs, num_samples: 5)
samples
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  s64[3][5]
  EXLA.Backend<host:0, 0.3254234594.1816002575.241381>
  [
    [1, 4, 0, 4, 4],
    [2, 1, 1, 2, 4],
    [4, 4, 4, 4, 4]
  ]
>
```

## Generating text with the bigram model, w/ multinomial

Let's see what happens if we use multinomial now.

```elixir
generate_fn = fn model, params, init_seq, key, max_new_tokens ->
  Enum.reduce(1..max_new_tokens, {key, init_seq}, fn _i, {key, acc} ->
    {_b, t} = Nx.shape(acc)

    # Cap the input sequence length from [t, block size]
    context_length = min(t, block_size)
    context_range = -context_length..-1
    context_slice = acc[[.., context_range]]

    # Predict next batch of chars (when we generate text, batch_size = 1)
    preds = Axon.predict(model, params, context_slice)
    logits = preds[[.., -1, ..]]
    probs = Axon.Activations.softmax(logits)
    {next_key, batch_char} = RandomPlus.multinomial(key, probs, num_samples: 1)

    {next_key, Nx.concatenate([acc, batch_char], axis: -1)}
  end)
  |> then(fn {_next_key, acc} -> acc end)
end

init_seq = Nx.broadcast(0, {1, 1})
key = Nx.Random.key(1337)
max_new_tokens = 1000

generate_fn.(bigram_model, params, init_seq, key, max_new_tokens)
|> Nx.to_list()
|> Enum.map(fn encoded_list -> Minidecoder.decode(encoded_list) end)
|> List.first()
|> IO.puts()
```

<!-- livebook:{"output":true} -->

```

S:
'S: ses t Pis ave, lef. j'ICay thiles Won.Beroliveng'llofrrouseff he:
Tho ff wous ke
Ker!
lthean areel.
Whon ofilok Alil t thom.Xe;
K&C, tid kinside mou.
TrvicQu ous d mitesors; YCAns qgar matamES:
RISshore qu. ue thuspipKI K:
We che. nd y manthamean the fo,
Ar!
I oout. cowieayouroflllothalveedrgrme
d patit f 3
CK har

UELl;
If chankn ourinowoftipor hendvis?u,
WAgorvan;
Ho hos,
EvS:
TOVck fodonrQ$&-bQhons s her, 'd.

tyolatoresces of$Qy; opy thTho fopum f.
CHNRUCres meowea d s
Thetsos on psth or
PRecon limy t t:
's by indngrs, pXG bntr f hs:
Thays thomea ELESSus cs; at,
t sanLIt,
MI he:
RElide, oppratrmarorige wW:
I as baiak t eind,
HF&CHAtinNIUSI thenane ou I ke hou arou speou!ARE pere at t my ba'3 h brmin ntr alt;
FLenurthourarait:

Hor yo h ctind hadinot:
I theoher. tal t is gx?ws o-
CNRI: tetigQMRRIthe useer be st ost's wn.
CAifls frin tsovim athe her; ys:
'dWhteyorexBy d y it wegeangr thur y patit, ou.MI3.

Austhar
Gralie:
Wherrvjul; fise s d arerve be'g;
HZXINGinghy!q&GMBY lce
```

<!-- livebook:{"output":true} -->

```
:ok
```

This looks somewhat better with our limited training. We'll improve the text generation by focusing on single-head and multi-head attention next.

Before we implement the attention models, let's create a reusable text generation function for different block sizes (sequence lengths). block_size is required to cap the sequence context for each prediction.

```elixir
defmodule TextGen do
  def generate(model, params, init_seq, block_size, opts \\ []) do
    opts = Keyword.validate!(opts, key_seed: 1337, max_new_tokens: 1000)

    key = opts[:key_seed] |> Nx.Random.key()
    max_new_tokens = opts[:max_new_tokens]

    Enum.reduce(1..max_new_tokens, {key, init_seq}, fn _i, {key, acc} ->
      {_b, t} = Nx.shape(acc)

      # Cap the input sequence length from [t, block size]
      context_length = min(t, block_size)
      context_range = -context_length..-1
      context_slice = acc[[.., context_range]]

      # Predict next batch of chars (but for us, batch_size = 1)
      preds = Axon.predict(model, params, context_slice)
      logits = preds[[.., -1, ..]]
      probs = Axon.Activations.softmax(logits)
      {next_key, batch_char} = RandomPlus.multinomial(key, probs, num_samples: 1)

      {next_key, Nx.concatenate([acc, batch_char], axis: -1)}
    end)
    |> then(fn {_next_key, acc} -> acc end)
    # Convert our Nx.tensor to Elixir list
    |> Nx.to_list()
    # Decode the results
    |> Enum.map(fn encoded_list -> Minidecoder.decode(encoded_list) end)
    # Our input just 1 batch, so grab the first one
    |> List.first()
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, TextGen, <<70, 79, 82, 49, 0, 0, 15, ...>>, {:generate, 5}}
```

## The mathematical trick to self attention (version #4)

To implement attention like how Karpathy does it, we'll create lower triangular matrices filled with ones. Nx doesn't have an equivalent `torch.tril`, but we can create these matrices using the iota function.

The iota function is commonly used to create tensors with consecutive values, starting from a specified value and incrementing by one. We can leverage this to create two tensors (row_iota and column_iota) and compare them to create the attention mask.

```elixir
shape = {3, 3}
row_iota = Nx.iota(shape, axis: 0)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  s64[3][3]
  EXLA.Backend<host:0, 0.3254234594.1812594704.219251>
  [
    [0, 0, 0],
    [1, 1, 1],
    [2, 2, 2]
  ]
>
```

```elixir
column_iota = Nx.iota(shape, axis: 1)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  s64[3][3]
  EXLA.Backend<host:0, 0.3254234594.1812594704.219253>
  [
    [0, 1, 2],
    [0, 1, 2],
    [0, 1, 2]
  ]
>
```

```elixir
Nx.greater_equal(row_iota, column_iota)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  u8[3][3]
  EXLA.Backend<host:0, 0.3254234594.1812594704.219255>
  [
    [1, 0, 0],
    [1, 1, 0],
    [1, 1, 1]
  ]
>
```

```elixir
defmodule Tril do
  import Nx.Defn

  # Creates a lower triangular matrix of 1s to use as our mask 
  defn ones(opts \\ []) do
    assert_keys(opts, [:shape])

    shape = opts[:shape]
    Nx.greater_equal(Nx.iota(shape, axis: 0), Nx.iota(shape, axis: 1))
  end
end

Tril.ones(shape: {5, 5})
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  u8[5][5]
  EXLA.Backend<host:0, 0.3254234594.1812594704.219261>
  [
    [1, 0, 0, 0, 0],
    [1, 1, 0, 0, 0],
    [1, 1, 1, 0, 0],
    [1, 1, 1, 1, 0],
    [1, 1, 1, 1, 1]
  ]
>
```

## The mathematical trick to self attention (version #4) cont.

Here's a rough draft of how attention is computed in a single head. We'll package this up later into a reusable layer

```elixir
{b, t, c} = {4, 8, 32}
{x, key} = Nx.Random.normal(Nx.Random.key(1337), shape: {b, t, c}, type: :f32)

head_size = 16
# Used for initializing random key, query, value kernels
keys = key |> Nx.Random.split(parts: 3)

# For some reason the default scale (2.0) produces really high weight values
init_fn = Axon.Initializers.he_uniform(scale: 0.5)

key_kernel = init_fn.({c, head_size}, {:f, 32}, keys[0])
query_kernel = init_fn.({c, head_size}, {:f, 32}, keys[1])
value_kernel = init_fn.({c, head_size}, {:f, 32}, keys[2])
k = Axon.Layers.dense(x, key_kernel)
q = Axon.Layers.dense(x, query_kernel)
v = Axon.Layers.dense(x, value_kernel)
kT = Nx.transpose(k, axes: [0, -1, -2])

# {b, t, t}
wei = Nx.dot(q, [2], [0], kT, [1], [0])

# Broadcast tril to {b, t, t} for Nx.select
tril = Tril.ones(shape: {t, t})
tril = Nx.broadcast(tril, {b, t, t})

# Broadcast neg_inf to {b, t, t} for Nx.select
wei_type = Nx.type(wei)
neg_inf = Nx.broadcast(Nx.Constants.neg_infinity(wei_type), wei)

# lower triangular part of wei has original values
# upper triangular part of wei has -neg_inf for its values
wei = Nx.select(tril, wei, neg_inf)

# {4, 8, 8}
wei = Axon.Activations.softmax(wei, axis: -1)

# {4,8,8} @ {4,8,16}
# out = Nx.dot(wei, [-1], [0], v, [1], [0])
# wei[0]
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[4][8][8]
  EXLA.Backend<host:0, 0.3254234594.1812594704.220883>
  [
    [
      [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
      [0.5236416459083557, 0.4763583540916443, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
      [0.1460239738225937, 0.8355455994606018, 0.018430398777127266, 0.0, 0.0, 0.0, 0.0, 0.0],
      [0.2382892519235611, 0.19527707993984222, 0.5128787755966187, 0.053554967045784, 0.0, 0.0, 0.0, 0.0],
      [0.23525264859199524, 0.02861269935965538, 0.12083742767572403, 0.43159112334251404, 0.18370607495307922, 0.0, 0.0, 0.0],
      [0.4105880558490753, 0.009884358383715153, 0.19398914277553558, 9.765126160345972e-4, 0.37614840269088745, 0.008413595147430897, 0.0, 0.0],
      [0.4806952476501465, 0.09604756534099579, ...],
      ...
    ],
    ...
  ]
>
```

## Single-head attention layer

Let's package up the computation we did earlier into an Axon layer. Since we need to do some custom calculations, we'll use `Axon.layer`, you can learn more about custom layers in the [Axon docs](https://hexdocs.pm/axon/custom_layers.html)

```elixir
defmodule SingleAttention do
  import Nx.Defn

  def head_layer(%Axon{} = key, %Axon{} = query, %Axon{} = value, opts \\ []) do
    Axon.layer(&head_layer_impl/4, [key, query, value],
      name: opts[:name],
      op_name: :single_head
    )
  end

  defn head_layer_impl(k, q, v, _opts \\ []) do
    {_b, t, c} = Nx.shape(k)
    tensor_type = Nx.type(k)

    kT = Nx.transpose(k, axes: [0, -1, -2])

    # {4,8,_16_} @ {4,_16_,8} = {4,8,8}
    wei = Nx.dot(q, [2], [0], kT, [1], [0])

    # Scaled attention
    wei = wei * Nx.rsqrt(c)

    # attention masking
    tril = Tril.ones(shape: {t, t})
    tril = Nx.broadcast(tril, wei)
    neg_inf = Nx.broadcast(Nx.Constants.neg_infinity(tensor_type), wei)
    # tril, wei, and neg_inf have the shape {b, t, t}
    wei = Nx.select(tril, wei, neg_inf)
    wei = Axon.Activations.softmax(wei, axis: -1)

    Nx.dot(wei, [-1], [0], v, [1], [0])
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, SingleAttention, <<70, 79, 82, 49, 0, 0, 18, ...>>, true}
```

## Single-head attention model

The general flow of this simple single-head attention model goes like this

* input sequence is some encoded text
* map our input into some n_embd dimensional space
* map some position information into some n_embd dimensional space
  * note: GPT uses sine and cosine functions for positional encoding. we won't be implementing that
  * our position information is just some iota tensor with values `[0..t)` where t == sequence length of our input
* add the tensors to produce a tensor filled with embedding + position information, we'll call this tensor `x`
* feed x into three different layers: key, value, and query
* compute self attention (this is complicated, this [youtube video](https://www.youtube.com/watch?v=g2BRIuln4uc) provides a great explanation)
  * note: For our single-head attention model, the size of the attention head is equal to n_embd.
* project self attention output down to vocab_size. tensors coming out of this layer will look like `{b,t,vocab_size}`. These are our logits.

Note: The implementation of the positional_embedding_table is a bit hacky. If somebody knows a better solution, I'd be curious to hear about it.

```elixir
# Hyperparameters from https://youtu.be/kCc8FmEb1nY?t=4907
batch_size = 32
block_size = 8
n_embd = 32
head_size = n_embd

# Model definition
input = Axon.input("sequence")

token_embedding_table =
  input
  |> Axon.embedding(vocab_size, n_embd)

# Generate positional encodings for the input sequence (hacky)
positions =
  Axon.nx(input, fn input ->
    {_batch_size, sequence_length} = Nx.shape(input)
    Nx.iota({sequence_length})
  end)

# Positional encodings get mapped into @n_embd space
position_embedding_table =
  Axon.embedding(positions, block_size, n_embd, name: "position_embedding")

x_layer = Axon.add(token_embedding_table, position_embedding_table)

he_uniform = Axon.Initializers.he_uniform(scale: 0.5)
key = x_layer |> Axon.dense(head_size, kernel_initializer: he_uniform, name: "key")
query = x_layer |> Axon.dense(head_size, kernel_initializer: he_uniform, name: "query")
value = x_layer |> Axon.dense(head_size, kernel_initializer: he_uniform, name: "value")

single_head_model =
  SingleAttention.head_layer(key, query, value)
  |> Axon.dense(vocab_size, kernel_initializer: :he_uniform, name: "language_modeling_head")
```

<!-- livebook:{"output":true} -->

```
#Axon<
  inputs: %{"sequence" => nil}
  outputs: "language_modeling_head"
  nodes: 11
>
```

## Single-head attention model graph

```elixir
Axon.Display.as_graph(single_head_model, Nx.template({batch_size, block_size}, :f32),
  direction: :top_down
)
```

<!-- livebook:{"output":true} -->

```mermaid
graph TD;
13[/"sequence (:input) {32, 8}"/];
14["embedding_0 (:embedding) {32, 8, 32}"];
15["nx_0 (:nx) {8}"];
16["position_embedding (:embedding) {8, 32}"];
17["container_0 (:container) {{32, 8, 32}, {8, 32}}"];
18["add_0 (:add) {32, 8, 32}"];
19["key (:dense) {32, 8, 32}"];
20["query (:dense) {32, 8, 32}"];
21["value (:dense) {32, 8, 32}"];
22["single_head_0 (:single_head) {32, 8, 32}"];
23["language_modeling_head (:dense) {32, 8, 65}"];
22 --> 23;
21 --> 22;
20 --> 22;
19 --> 22;
18 --> 21;
18 --> 20;
18 --> 19;
17 --> 18;
16 --> 17;
14 --> 17;
15 --> 16;
13 --> 15;
13 --> 14;
```

## Training the single-head attention model

I lowered the iterations just to speed up training on my machine. You can increase the numbers to get better results.

```elixir
{init_fn, predict_fn} = Axon.build(single_head_model, mode: :train)
custom_predict_fn = &CommonTrain.custom_predict_fn(predict_fn, &1, &2)
custom_loss_fn = &CommonTrain.custom_loss_fn(&1, &2)
train_data_stream = get_batch_stream.(batch_size, block_size, :train)

params =
  {init_fn, custom_predict_fn}
  |> Axon.Loop.trainer(custom_loss_fn, Axon.Optimizers.adamw())
  |> Axon.Loop.run(train_data_stream, %{}, epochs: 1, iterations: 3000, compiler: EXLA)
```

<!-- livebook:{"output":true} -->

```

16:25:33.984 [debug] Forwarding options: [compiler: EXLA] to JIT compiler
Epoch: 0, Batch: 2950, loss: 2.6138554
```

<!-- livebook:{"output":true} -->

```
%{
  "embedding_0" => %{
    "kernel" => #Nx.Tensor<
      f32[65][32]
      EXLA.Backend<host:0, 0.3254234594.1814954000.85583>
      [
        [-0.12449812889099121, -0.041491199284791946, 0.015664780512452126, 0.03799212723970413, -0.31947198510169983, -0.05881452187895775, 0.1857747584581375, 0.3325536847114563, -0.041606295853853226, 0.026597503572702408, 0.19090507924556732, -0.3752489686012268, -0.27863290905952454, 0.05212874710559845, 0.021286770701408386, -0.19017533957958221, 0.42697465419769287, -0.2521313428878784, -0.23345819115638733, -0.0558040551841259, 0.35665449500083923, -0.0012820062693208456, -0.36729896068573, -0.14818602800369263, -0.06106257438659668, -0.11971031129360199, 0.17516298592090607, 0.15441229939460754, 0.42313143610954285, 0.18144471943378448, 0.09868703037500381, 0.1866658478975296],
        [0.02428605780005455, 0.09882549196481705, 0.05271231383085251, -0.35898908972740173, -0.4262569546699524, 0.0750197321176529, 0.08986613899469376, 0.26255369186401367, 0.20215867459774017, 0.04128522053360939, 0.1987818330526352, -0.19247201085090637, -0.4218704402446747, 0.3209207355976105, -0.17211270332336426, 0.1550392061471939, ...],
        ...
      ]
    >
  },
  "key" => %{
    "bias" => #Nx.Tensor<
      f32[32]
      EXLA.Backend<host:0, 0.3254234594.1814954000.85584>
      [6.951812538318336e-4, 1.0656993254087865e-4, -2.6717042783275247e-4, -4.899475025013089e-4, 1.609715836821124e-4, -3.93563968827948e-4, -2.8729886253131554e-5, 2.411991445114836e-4, -8.811319712549448e-4, 2.112942311214283e-4, 0.0010914984159171581, 5.786920664831996e-4, -2.053074240393471e-5, 3.5198236582800746e-4, 3.915879060514271e-4, -2.9979119062772952e-5, 6.31617585895583e-5, -5.291543784551322e-4, -9.32441107579507e-5, 2.2229686146602035e-4, 7.390856044366956e-4, -0.0011152713559567928, -1.041940413415432e-4, 6.499612936750054e-5, 2.5813374668359756e-4, 6.661629595328122e-5, -4.07946681661997e-5, -5.688820965588093e-4, -3.065955825150013e-4, 2.612621756270528e-4, 8.065305883064866e-4, -4.968871362507343e-4]
    >,
    "kernel" => #Nx.Tensor<
      f32[32][32]
      EXLA.Backend<host:0, 0.3254234594.1814954000.85585>
      [
        [0.33975404500961304, -0.30582377314567566, -0.8231735229492188, 0.19240649044513702, -0.12287508696317673, -0.7093309760093689, 0.2760106027126312, -0.5405341386795044, -0.7421066164970398, -0.4193912744522095, 0.7165603637695312, 0.7408425807952881, -0.4670274257659912, -0.35215866565704346, 0.7326067090034485, -0.528164803981781, 0.6684748530387878, 0.7050886154174805, -0.5397908091545105, 0.6194822192192078, 0.7474294900894165, -0.6709204912185669, -0.466318815946579, -0.7617844343185425, -0.5371933579444885, -0.03849504888057709, -0.36445751786231995, -0.66940838098526, 0.6848836541175842, -0.08551371842622757, -0.3973628580570221, -0.10625246167182922],
        [-0.1320735663175583, 0.18312077224254608, 0.12171944230794907, -0.10132884234189987, 0.07501734793186188, 0.10778805613517761, 0.23268064856529236, -0.05522453412413597, 0.2936646044254303, 0.013814796693623066, -0.12851369380950928, -0.2725337743759155, 0.14671465754508972, 0.3485184907913208, ...],
        ...
      ]
    >
  },
  "language_modeling_head" => %{
    "bias" => #Nx.Tensor<
      f32[65]
      EXLA.Backend<host:0, 0.3254234594.1814954000.85586>
      [-0.05685506761074066, 0.07064026594161987, -0.2557661831378937, -0.2560362219810486, -0.26968517899513245, -0.03484046459197998, -0.07731270045042038, -0.1036960631608963, -0.2804858386516571, -0.2759052813053131, 0.05126965790987015, -0.31376343965530396, -0.24393656849861145, -0.05841764435172081, 0.025193942710757256, 0.07555720955133438, 0.0926930233836174, 0.15460146963596344, -0.07690055668354034, -0.019845766946673393, -0.0235418900847435, 0.09289442002773285, -0.15384411811828613, -0.1338532418012619, 0.12292876839637756, 3.086665237788111e-4, 0.026255056262016296, 0.1270291805267334, 0.014981858432292938, -0.19156759977340698, 0.11861748993396759, -0.002292182296514511, -0.10909105837345123, 0.03924639895558357, -0.10732898861169815, 0.01231068093329668, -0.1728232353925705, 0.08287977427244186, -0.16030320525169373, 0.033048901706933975, -0.05948251485824585, -0.01520210225135088, 0.1463228464126587, 0.06396275758743286, 0.04926832392811775, -0.03303537517786026, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[32][65]
      EXLA.Backend<host:0, 0.3254234594.1814954000.85587>
      [
        [0.3055857717990875, 0.5028261542320251, -0.6969407200813293, -0.9272090792655945, -0.659098744392395, 0.21539360284805298, -0.3290468156337738, 0.3705507814884186, -0.28210166096687317, -1.0550962686538696, 0.06689183413982391, -0.5167394280433655, -0.7136815190315247, -0.20929455757141113, -0.05115950107574463, 0.1114414781332016, 0.03598397225141525, -0.15534289181232452, 0.1699269860982895, -0.3447176516056061, 0.014579200185835361, 0.16929131746292114, -0.25948068499565125, -0.4538488984107971, 0.1307125836610794, 0.2259068787097931, 0.31830158829689026, -0.23598317801952362, 0.08688582479953766, -0.03220289945602417, -0.2709537744522095, -0.11591695249080658, 0.3041294813156128, -0.7768739461898804, -0.26381343603134155, 0.2610306739807129, -0.20942726731300354, -0.07676354795694351, -0.42694589495658875, -0.3091547191143036, 0.29672303795814514, 0.3014737069606781, 0.6439229249954224, -0.18490462005138397, 0.07470633089542389, ...],
        ...
      ]
    >
  },
  "position_embedding" => %{
    "kernel" => #Nx.Tensor<
      f32[8][32]
      EXLA.Backend<host:0, 0.3254234594.1814954000.85588>
      [
        [0.026374930515885353, -0.15728774666786194, -0.04601259529590607, 0.17041920125484467, 0.121708445250988, -0.1601133495569229, -0.15654057264328003, 0.0060270014218986034, 0.19322821497917175, -0.06556583940982819, 0.09796100854873657, 0.025487519800662994, 0.09840478748083115, 0.34446343779563904, -0.028935175389051437, 0.16603080928325653, 0.0999445766210556, 0.12753452360630035, 0.020149223506450653, -0.06776680797338486, 0.029917173087596893, -0.00727547612041235, 0.04919647052884102, -0.08667118847370148, -0.12986767292022705, 0.002028305782005191, 0.24070723354816437, -0.0445316880941391, -0.052695728838443756, -0.11081618815660477, 0.07322117686271667, 0.03407822549343109],
        [0.0962025448679924, -0.10378734767436981, 0.030110664665699005, 0.11771107465028763, 0.064508356153965, -0.1586683839559555, -0.12807808816432953, 0.009951402433216572, 0.19166921079158783, 0.01996196061372757, 0.008448931388556957, 0.00941395852714777, 0.06996167451143265, ...],
        ...
      ]
    >
  },
  "query" => %{
    "bias" => #Nx.Tensor<
      f32[32]
      EXLA.Backend<host:0, 0.3254234594.1814954000.85589>
      [0.22771382331848145, 0.045823413878679276, -0.24893715977668762, -0.0273751150816679, 0.5336333513259888, 0.08926630765199661, -0.10543708503246307, -0.10000376403331757, -0.3058110475540161, 0.47543591260910034, 0.17458400130271912, -0.023093443363904953, 0.050433479249477386, 0.2753804922103882, 0.244731143116951, 0.16212639212608337, 0.17309950292110443, 0.3223939836025238, 0.32267892360687256, 0.19082072377204895, 0.5166218280792236, -0.3436724543571472, 0.22598545253276825, 0.12996748089790344, 0.2070816159248352, 0.14451278746128082, -0.2219020128250122, 0.04276592284440994, 0.32143712043762207, 0.36945948004722595, -0.2199181318283081, 0.2524649500846863]
    >,
    "kernel" => #Nx.Tensor<
      f32[32][32]
      EXLA.Backend<host:0, 0.3254234594.1814954000.85590>
      [
        [0.40876513719558716, -0.0890410989522934, -0.4595774710178375, 0.0022638996597379446, 0.5664355754852295, 0.19244565069675446, 0.038730498403310776, -0.22298948466777802, -0.5758785009384155, 0.28438031673431396, 0.0807274654507637, -0.0069666155613958836, 0.08125487715005875, 0.13316220045089722, 0.503198504447937, -0.015876319259405136, 0.5608922243118286, 0.39386817812919617, 0.2429884523153305, 0.19607770442962646, 0.7130902409553528, -0.5552664995193481, 0.14693580567836761, 0.24283038079738617, 0.004645336419343948, -0.22488436102867126, -0.45686236023902893, -0.20874078571796417, 0.4174264669418335, 0.19270040094852448, -0.4568701684474945, -0.12644438445568085],
        [-0.6498653888702393, 0.05695941299200058, 0.5432588458061218, -0.07167483121156693, 0.06145748123526573, 0.26113834977149963, 0.044911641627550125, 0.2763245701789856, 0.7895528078079224, -0.2564004361629486, -0.6150042414665222, ...],
        ...
      ]
    >
  },
  "value" => %{
    "bias" => #Nx.Tensor<
      f32[32]
      EXLA.Backend<host:0, 0.3254234594.1814954000.85591>
      [0.25676605105400085, 0.06779497861862183, -0.015785938128829002, -0.0695415735244751, -0.12299560755491257, 0.1962602436542511, -0.012755758129060268, -0.13441821932792664, 0.04693962633609772, -0.05062705650925636, 0.08340007066726685, 0.004149039275944233, 0.07942236959934235, -0.06921767443418503, -0.05694010481238365, 0.029614433646202087, -0.09284859895706177, -0.14327490329742432, -0.12092500925064087, -0.032240018248558044, 0.029213955625891685, -0.10157214105129242, 0.010117841884493828, -0.0877854973077774, -0.10096361488103867, -0.07226494699716568, -0.021919241175055504, 0.034811876714229584, 0.17195023596286774, -0.016491541638970375, 0.0398966446518898, 0.12043971568346024]
    >,
    "kernel" => #Nx.Tensor<
      f32[32][32]
      EXLA.Backend<host:0, 0.3254234594.1814954000.85592>
      [
        [0.537727952003479, -0.1983923465013504, -0.3481990694999695, -0.30246254801750183, 0.03715481609106064, 0.2642310559749603, -0.12971770763397217, -0.2975056767463684, 0.30819016695022583, 0.3726482391357422, 0.30987006425857544, 0.09532740712165833, 0.2216898798942566, -0.4953192174434662, -0.08318483084440231, -0.14217165112495422, -0.09140422195196152, -0.2326686829328537, -0.15457046031951904, -0.20497041940689087, -0.07046232372522354, -0.28149592876434326, 0.353458970785141, -0.3088686764240265, 0.04118921980261803, -0.41010481119155884, -0.27988481521606445, -0.031260136514902115, -0.3212616443634033, -0.1102636530995369, -0.3375175893306732, -0.06334506720304489],
        [0.05756823346018791, -0.019996583461761475, -0.2810303866863251, 0.4659218490123749, 0.08381642401218414, 0.33989089727401733, 0.16706013679504395, -0.4150942265987396, -0.37861737608909607, 0.5893587470054626, ...],
        ...
      ]
    >
  }
}
```

## Generating text w/ single-head attention model

```elixir
init_seq = Nx.broadcast(0, {1, 1})

TextGen.generate(single_head_model, params, init_seq, block_size, max_new_tokens: 1000)
|> IO.puts()
```

<!-- livebook:{"output":true} -->

```

S:
Bacroou t You bu heane th biry seo my awilseso.
Wene mlodoustohee ind ser fiavito le
Mnocetrind I meclcours.

An's R
KI:
Werer byon l d
Lou henave s mou istto Lu jus f mithours.

BAgr pncmanconagi!
 desesed prato forusone ha bes, che'seagu mante he, burie s ait d argowat trieayourr.

Sish, winesesio wem thich B'd Be p at fr fes d bepr.
 poouts wiorie.

Futint at noru k d t hot.

Y MIRESRLALODYLof Man stes s her h fie yss fourdol t n'd y;
PYumthart hintaald
FO:
TIort ment, AH:

Thesthe on prrided
Sater?

LAndicomf t,
he:
I roulng h bout hant, se ws tholfa I danus doeadn Iuarfe hy ble he'dd th pe'senimstharsen have he bre aiak t fle; d g d byon'llld this he ou Md p hou arsi, this on-
VIONE:
An ly ba a id sis,
Oto aly falosurthour, have Mere yo h dothe hadinot me then mofouim t he hy yt o O:
O bus, t gbirexle useas
Be rt ngo r wild Pifou hoth sthouf athe hat nyonce, stayorexay f y ho whe, ist thur y olous oro, he f droun n homar hatherputre harre s f arist ag!

CARINGRIRINIint ge i mal
```

<!-- livebook:{"output":true} -->

```
:ok
```

## Going from single head to multi head attention

If you're following along with the video, we're currently at this [part.](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=4919s)

Karpathy uses the following OOP code to create multiple heads of attention, but we can do the same thing by reshaping our original key, query, and value layer.

```
self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])
```

### Reshaping our key, query, value layers

Remember our single-head attention layer had k,q,v of size `{b, t, n_embd}`. In order to compute multi-head attention, we'll split this `n_embd` portion into `{n_head, head_size}`. (head_size is known as hidden_size in other places like Bumblebee). This lets us view the tensor as having multiple heads. For example, let x be some key/query/value tensor.

1. `{b,t,n_embd} = Nx.shape(x)`
2. Reshape the last axis of x to become `{b,t,n_heads,head_size}`
3. Transpose x so that the `t` and `n_heads` axes become swapped `{b, n_heads, t, head_size}`
4. x is now ready to multiply with some other key, query, value tensor that has also undergone this reshaping. When `{b, n_heads, t, head_size}` @ `{b, n_heads, t, head_size}`, it'll produce a tensor of shape `{b, n_heads, t, t}`. This `{t,t}` portion is where we apply our attention mask.

This is also how it's done in the Bumblebee library

* https://github.com/elixir-nx/bumblebee/blob/main/lib/bumblebee/layers.ex#L525
* https://github.com/elixir-nx/bumblebee/blob/main/lib/bumblebee/layers.ex#L194
* https://github.com/elixir-nx/bumblebee/blob/main/lib/bumblebee/layers.ex#L220

```elixir
# simple multi-head attention without any optimizations like layernorm / feedforward
defmodule MultiAttention do
  import Nx.Defn

  @doc """
  Modified from Bumblebee's transformer.ex 
  https://github.com/elixir-nx/bumblebee/blob/main/lib/bumblebee/layers.ex#L525

  Splits the hidden dimension into the given number of attention heads.

  In other words, the input with shape `{batch_size, sequence_length, hidden_size}`
  is reshaped to `{batch_size, sequence_length, num_heads, hidden_size}`.
  Then, transposed to `{batch_size, num_heads, sequence_length, *}` 
  """
  def split_heads(states, num_heads, opts \\ []) do
    opts = Keyword.validate!(opts, name: "split_heads")

    Axon.nx(
      states,
      fn states ->
        batch_size = Nx.axis_size(states, 0)
        sequence_length = Nx.axis_size(states, 1)
        new_shape = {batch_size, sequence_length, num_heads, :auto}

        states
        |> Nx.reshape(new_shape)
        |> Nx.transpose(axes: [0, 2, 1, 3])
      end,
      name: opts[:name]
    )
  end

  def multi_head_layer(%Axon{} = x, num_heads, head_size, opts \\ []) do
    default_initializer = Axon.Initializers.he_uniform(scale: 0.5)
    opts = Keyword.validate!(opts, kernel_initializer: default_initializer)
    initializer = opts[:kernel_initializer]

    key =
      x
      |> Axon.dense(num_heads * head_size, kernel_initializer: initializer, name: "key")
      |> split_heads(num_heads)

    query =
      x
      |> Axon.dense(num_heads * head_size, kernel_initializer: initializer, name: "query")
      |> split_heads(num_heads)

    value =
      x
      |> Axon.dense(num_heads * head_size, kernel_initializer: initializer, name: "value")
      |> split_heads(num_heads)

    Axon.layer(&multi_head_layer_impl/4, [key, query, value], name: "multi_head_attention")
  end

  # Custom layers require the opts argument
  # https://hexdocs.pm/axon/custom_layers.html#creating-custom-layers
  defn multi_head_layer_impl(k, q, v, _opts \\ []) do
    {b, h, t, c} = Nx.shape(k)
    tensor_type = Nx.type(k)

    # {b, h, t, c} @ {b, h, c, t} -> {b, h, t, t}
    wei = Nx.dot(q, [3], [0, 1], k, [3], [0, 1])

    # Scaled attention
    wei = wei * Nx.rsqrt(c)

    # Attention masking
    tril = Tril.ones(shape: {t, t})
    tril = Nx.broadcast(tril, wei)
    neg_inf = Nx.broadcast(Nx.Constants.neg_infinity(tensor_type), wei)
    wei = Nx.select(tril, wei, neg_inf)
    wei = Axon.Activations.softmax(wei, axis: -1)

    # {b, h, t, t} @ {b, h, t, head_size} -> {b, h, t, head_size}
    out = Nx.dot(wei, [3], [0, 1], v, [2], [0, 1])

    # Tranpose so we can stack the heads on top of each other
    # {b, h, t, c} -> {b, t, h, c}
    out = Nx.transpose(out, axes: [0, 2, 1, 3])

    # Our output tensor is now enriched with attention information
    # We shape it back to {b, t, c}
    # This gives us the proper shape to add to our original input x
    Nx.reshape(out, {b, t, h * c})
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, MultiAttention, <<70, 79, 82, 49, 0, 0, 25, ...>>, true}
```

## Multi-head attention model

```elixir
# We'll reuse the hyperparameters from single-head attention model but change head size 
# batch_size = 32
# block_size = 8
# n_embd = 32
n_heads = 4
head_size = div(n_embd, n_heads)

multi_head_model =
  Axon.input("sequence")
  |> then(fn input ->
    # Create an embedding for the input data
    token_embedding_table = Axon.embedding(input, vocab_size, n_embd, name: "token_embedding")

    # Generate positional encodings for the input sequence (hacky, couldn't find alternative)
    positions =
      Axon.nx(input, fn input ->
        {_batch_size, sequence_length} = Nx.shape(input)
        Nx.iota({sequence_length})
      end)

    # Positional encodings get mapped into @n_embd space
    position_embedding_table =
      Axon.embedding(positions, block_size, n_embd, name: "position_embedding")

    # Add the two layers above to produce tensors containing embedding + position info
    Axon.add(token_embedding_table, position_embedding_table, name: "x_positional_encoding")
  end)
  |> MultiAttention.multi_head_layer(n_heads, head_size)
  |> Axon.dense(vocab_size, kernel_initializer: :he_uniform, name: "language_modeling_head")
```

<!-- livebook:{"output":true} -->

```
#Axon<
  inputs: %{"sequence" => nil}
  outputs: "language_modeling_head"
  nodes: 14
>
```

## Multi-head attention model graph

Notice how the key, query, value layers split to become 4-dimensional.

```elixir
Axon.Display.as_graph(multi_head_model, Nx.template({batch_size, block_size}, :f32),
  direction: :top_down
)
```

<!-- livebook:{"output":true} -->

```mermaid
graph TD;
24[/"sequence (:input) {32, 8}"/];
25["token_embedding (:embedding) {32, 8, 32}"];
26["nx_0 (:nx) {8}"];
27["position_embedding (:embedding) {8, 32}"];
28["container_0 (:container) {{32, 8, 32}, {8, 32}}"];
29["x_positional_encoding (:add) {32, 8, 32}"];
30["key (:dense) {32, 8, 32}"];
31["split_heads (:nx) {32, 4, 8, 8}"];
32["query (:dense) {32, 8, 32}"];
33["split_heads (:nx) {32, 4, 8, 8}"];
34["value (:dense) {32, 8, 32}"];
35["split_heads (:nx) {32, 4, 8, 8}"];
36["multi_head_attention (:custom) {32, 8, 32}"];
37["language_modeling_head (:dense) {32, 8, 65}"];
36 --> 37;
35 --> 36;
33 --> 36;
31 --> 36;
34 --> 35;
29 --> 34;
32 --> 33;
29 --> 32;
30 --> 31;
29 --> 30;
28 --> 29;
27 --> 28;
25 --> 28;
26 --> 27;
24 --> 26;
24 --> 25;
```

## Training the multi-head attention model

```elixir
{init_fn, predict_fn} = Axon.build(multi_head_model, mode: :train)
custom_predict_fn = &CommonTrain.custom_predict_fn(predict_fn, &1, &2)
custom_loss_fn = &CommonTrain.custom_loss_fn(&1, &2)
train_data_stream = get_batch_stream.(batch_size, block_size, :train)

params =
  {init_fn, custom_predict_fn}
  |> Axon.Loop.trainer(custom_loss_fn, Axon.Optimizers.adamw())
  |> Axon.Loop.run(train_data_stream, %{}, epochs: 1, iterations: 3000, compiler: EXLA)
```

<!-- livebook:{"output":true} -->

```

16:34:09.194 [debug] Forwarding options: [compiler: EXLA] to JIT compiler
Epoch: 0, Batch: 2950, loss: 2.5592556
```

<!-- livebook:{"output":true} -->

```
%{
  "key" => %{
    "bias" => #Nx.Tensor<
      f32[32]
      EXLA.Backend<host:0, 0.3254234594.1817575440.157044>
      [-8.425533305853605e-4, -0.004405028652399778, 0.0010972011368721724, 0.004076654557138681, 5.405354895628989e-4, 0.004306342918425798, -9.779995307326317e-4, -0.004172821529209614, 0.0021942087914794683, -0.0016705284360796213, 0.003271385794505477, 0.0015083949547261, -7.86721589975059e-4, 0.002464361023157835, -0.0024935470428317785, -0.0019821603782474995, 5.474902573041618e-4, -0.001554747112095356, -0.005774300079792738, -0.003370372112840414, 0.004051737952977419, 0.0043303812853991985, -4.943694220855832e-4, 0.0036415462382137775, -5.83827611990273e-4, 0.001292707398533821, -6.238421192392707e-4, -4.070802533533424e-4, -0.0011379366042092443, 5.957871908321977e-4, -7.222724379971623e-4, -2.331361174583435e-4]
    >,
    "kernel" => #Nx.Tensor<
      f32[32][32]
      EXLA.Backend<host:0, 0.3254234594.1817575440.157045>
      [
        [0.01818872056901455, 0.7190983295440674, -0.433313250541687, -0.376583456993103, -0.042806509882211685, -0.46840494871139526, -0.06012649089097977, 0.7332191467285156, -0.467936635017395, 0.08437876403331757, 1.1533902883529663, -0.5669853091239929, -0.7957673072814941, 0.34184694290161133, -1.1306474208831787, -0.5669809579849243, -0.39337900280952454, 1.0103614330291748, 0.7785429954528809, 0.7425878047943115, -0.6526063084602356, -0.6811506152153015, -0.036152344197034836, -0.1701856255531311, -0.3277337849140167, -0.2869316041469574, 0.21203546226024628, 0.25570759177207947, 0.7052516341209412, -0.4254758954048157, 0.3701837360858917, 0.9926936626434326],
        [-0.12290152907371521, -0.7079461216926575, 0.9683096408843994, 0.6369351744651794, -0.1697685718536377, 0.25893399119377136, 0.4694293439388275, -0.7105491161346436, -0.12395697087049484, 0.0402412973344326, 0.0988023653626442, -0.5885880589485168, 0.5485720038414001, 0.30997416377067566, 0.13458476960659027, ...],
        ...
      ]
    >
  },
  "language_modeling_head" => %{
    "bias" => #Nx.Tensor<
      f32[65]
      EXLA.Backend<host:0, 0.3254234594.1817575440.157046>
      [0.030769379809498787, 0.16657856106758118, -0.2833971381187439, -0.24933601915836334, -0.23414017260074615, -0.12016116827726364, -0.1567125767469406, -0.11754469573497772, -0.2621855139732361, -0.20879793167114258, 0.07895071059465408, -0.26707515120506287, -0.2593075931072235, 0.003988867625594139, -0.11174427717924118, -0.02466743439435959, 0.027561591938138008, 0.09508165717124939, -0.06270306557416916, -0.03237610682845116, 0.004674181342124939, 0.17351622879505157, -0.12853550910949707, -0.06395868211984634, 0.12011461704969406, 0.024228468537330627, -0.02315649390220642, 7.243326399475336e-4, -0.0336749404668808, -0.18578380346298218, 0.1669282615184784, 0.012338697910308838, 0.11631806939840317, 0.1334124207496643, -0.10050827264785767, -0.07985969632863998, -0.19281582534313202, 0.001464550499804318, -0.17037969827651978, -0.03602610528469086, -0.09085739403963089, 0.06935708224773407, -0.0665714219212532, 0.05942726880311966, 0.06146202236413956, -0.03266090154647827, 0.0724366307258606, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[32][65]
      EXLA.Backend<host:0, 0.3254234594.1817575440.157047>
      [
        [-0.04678845405578613, 0.11248212307691574, 0.19060900807380676, -0.7219302654266357, -0.3279426395893097, 0.026258643716573715, -0.19837601482868195, -0.2807901203632355, 0.10336864739656448, -0.29430046677589417, 0.48107174038887024, 0.005581407342106104, -0.04817259684205055, 0.3677353262901306, -0.11676892638206482, -0.018021298572421074, 0.024292344227433205, 0.5273807644844055, -0.45499175786972046, 0.4540828466415405, 0.2362978309392929, 0.345051109790802, -0.38458317518234253, -0.23793229460716248, -0.3126399517059326, -0.5301098823547363, -0.48424386978149414, 0.2978005111217499, -0.3244752585887909, -0.37336409091949463, 0.16671335697174072, -0.4914577305316925, 0.07758120447397232, 0.07958077639341354, 0.14874085783958435, 0.0303023848682642, -0.22642405331134796, 0.03899882361292839, -0.12340163439512253, 0.567351222038269, 0.05619846656918526, -0.23822295665740967, -0.2675016522407532, 0.5342051386833191, 0.07797959446907043, -0.8944621086120605, ...],
        ...
      ]
    >
  },
  "position_embedding" => %{
    "kernel" => #Nx.Tensor<
      f32[8][32]
      EXLA.Backend<host:0, 0.3254234594.1817575440.157048>
      [
        [0.0013752954546362162, -0.17160192131996155, 0.09195833653211594, -0.05347657576203346, -0.03460952639579773, -0.04413750395178795, -0.21762704849243164, 0.07786468416452408, -0.06609530001878738, -0.2916222810745239, 0.10711128264665604, -0.017960157245397568, -0.07732363045215607, -0.012831684201955795, 0.19765445590019226, 0.004277145955711603, 0.39317455887794495, 0.030109109356999397, -0.38576528429985046, 0.07186834514141083, -0.0356307178735733, 0.2295781672000885, -0.2371460348367691, -0.25608983635902405, -0.16864502429962158, -0.019621388986706734, 0.0035512042231857777, -0.25996243953704834, 0.015051321126520634, -0.07492557913064957, -0.07170603424310684, -0.02204338274896145],
        [0.06624335795640945, -0.14573314785957336, 0.07876218110322952, -0.07147928327322006, -0.0037499875761568546, -0.018455546349287033, -0.22692157328128815, -0.018909981474280357, -0.08792765438556671, -0.23270949721336365, 0.04798690229654312, -0.08313611149787903, -0.04203498736023903, -0.10477416962385178, ...],
        ...
      ]
    >
  },
  "query" => %{
    "bias" => #Nx.Tensor<
      f32[32]
      EXLA.Backend<host:0, 0.3254234594.1817575440.157049>
      [-0.07243883609771729, 0.19255661964416504, 0.37171608209609985, -0.1803603172302246, -0.4135473370552063, -0.4621853828430176, 0.5973693132400513, 0.2583131194114685, -0.28009146451950073, -0.15813636779785156, 0.36175280809402466, -0.24274775385856628, 0.21189072728157043, 0.46984925866127014, -0.4331381320953369, 0.41732823848724365, 0.3539159893989563, -0.1295434981584549, 0.4968760907649994, 0.027348347008228302, -0.32529109716415405, -0.33312180638313293, -0.2270093560218811, -0.28348690271377563, 0.14710602164268494, -0.45558303594589233, 0.4042465090751648, -0.08544229716062546, 0.338629812002182, -0.39046528935432434, 0.47000300884246826, 0.14116282761096954]
    >,
    "kernel" => #Nx.Tensor<
      f32[32][32]
      EXLA.Backend<host:0, 0.3254234594.1817575440.157050>
      [
        [0.12167952954769135, 0.8109374642372131, -0.2596192955970764, -0.4434243440628052, -0.31913986802101135, -0.5968730449676514, 0.3275970220565796, 0.5807716846466064, -0.40895411372184753, 0.2506727874279022, 0.4332299530506134, -0.1031729057431221, 0.0824892595410347, 0.10905681550502777, -0.33720606565475464, 0.1411208063364029, -0.3026615083217621, 0.883449137210846, 0.9190484285354614, 0.6805400848388672, -0.425520122051239, -0.8671147227287292, 0.37636810541152954, -0.2162361890077591, -0.08758234977722168, -0.1692802906036377, -0.06505171209573746, 0.09252528846263885, 0.08090154081583023, -0.11841215938329697, 0.4467102885246277, 0.28756508231163025],
        [-0.13456404209136963, -0.6545833349227905, 0.5662257671356201, 0.7260304093360901, -0.31149953603744507, 0.3913393020629883, 0.10211214423179626, -0.7065038681030273, 0.19132934510707855, 0.3153495490550995, -0.3216674327850342, 0.19597099721431732, ...],
        ...
      ]
    >
  },
  "token_embedding" => %{
    "kernel" => #Nx.Tensor<
      f32[65][32]
      EXLA.Backend<host:0, 0.3254234594.1817575440.157051>
      [
        [-0.19927097856998444, 0.2815258502960205, -0.3459630310535431, 0.2590380311012268, 0.2532792091369629, -0.37395015358924866, -0.12921994924545288, 0.007705614902079105, 0.1729155331850052, -0.18092679977416992, 0.0032031042501330376, 0.08117944002151489, -0.40324634313583374, 0.1411435455083847, -0.5013056397438049, 0.34482434391975403, -0.19602452218532562, -0.24484454095363617, 0.10180579125881195, -0.3361901640892029, -0.32601869106292725, -0.003421568078920245, 0.08437026292085648, -7.765151094645262e-4, 0.4347735047340393, 0.21600857377052307, -0.3307442367076874, -0.3366284966468811, -0.13245777785778046, -0.14767013490200043, -0.0954267755150795, 0.38210511207580566],
        [0.2005457729101181, 0.40266913175582886, -0.07715561240911484, 0.011850405484437943, -0.03257988393306732, -0.5841193795204163, -0.3917044699192047, 0.14234112203121185, 0.5042423009872437, -0.5195887088775635, -0.2202458679676056, 0.3806746006011963, ...],
        ...
      ]
    >
  },
  "value" => %{
    "bias" => #Nx.Tensor<
      f32[32]
      EXLA.Backend<host:0, 0.3254234594.1817575440.157052>
      [0.1182149350643158, -0.0029432298615574837, -0.29579612612724304, -0.01666777767241001, 0.1204318255186081, 0.0681617259979248, -0.12144093960523605, 0.0991319790482521, -0.12692782282829285, 0.2618130147457123, -0.036400534212589264, -0.11615144461393356, -0.09769206494092941, 0.06890176236629486, 0.08943068236112595, -0.11467146873474121, -0.15157251060009003, 0.13185128569602966, 0.12522989511489868, 0.1292530745267868, 0.10252965241670609, 0.04526737332344055, 0.05440670996904373, 0.05254548043012619, 0.12204942852258682, 0.10538647323846817, 0.05214027315378189, 0.04546182602643967, -0.13132330775260925, 0.08356057852506638, -0.023249903693795204, -0.006252492778003216]
    >,
    "kernel" => #Nx.Tensor<
      f32[32][32]
      EXLA.Backend<host:0, 0.3254234594.1817575440.157053>
      [
        [-0.1679638922214508, 0.2237347811460495, 0.12955142557621002, -0.12170831859111786, 0.2780440151691437, 0.5078595876693726, 0.17197802662849426, 0.43497732281684875, -0.2821573317050934, -0.21377456188201904, 0.1346019059419632, -0.15031497180461884, -0.3035803735256195, -0.21402734518051147, 0.24235907196998596, -0.19840946793556213, -2.5668268790468574e-4, 0.43060415983200073, -0.13713760673999786, 0.2835043668746948, 0.10824284702539444, -0.3255954682826996, -0.021888984367251396, 0.09890792518854141, 0.09083979576826096, 0.08305889368057251, 0.13775111734867096, -0.12818510830402374, 0.0030468832701444626, 0.1257709562778473, -0.18476228415966034, 0.03390125930309296],
        [-0.3351919651031494, -0.04085490480065346, 0.3238913118839264, -0.21219518780708313, 0.20051291584968567, -0.0443987213075161, -0.06920430064201355, -0.3866513669490814, 0.19368185102939606, -0.5635990500450134, ...],
        ...
      ]
    >
  }
}
```

## Generating text w/ multi-head attention model

```elixir
init_seq = Nx.broadcast(0, {1, 1})

TextGen.generate(multi_head_model, params, init_seq, block_size, max_new_tokens: 1000)
|> IO.puts()
```

<!-- livebook:{"output":true} -->

```

TFI acoulokm Rou Wigand not Panex thimat ave with my so mou tomus, nalin sou gictous ke
Mus dst?

HAENINK:
Whor of com Bpil thiron win mag my,
His uras mso isttl If ous fanes wirpe al.
O
OOCK:
Gre bo, rertave.

UCLUELTHnsome ha!

Whaind the yed ith ghe buse fom cok mard:, thive ivheen sprris evindfesin wen too ga Ced
I p Lom the vain ism othannto word ancn: thet at osay ke it ith Ceve erene:
Hill tha,
Kast was her, Ane ayspbenon, les of ive's youn ot hout am?

ARIO wit noto k
Fas
Thestar om row.

I
OMULSE:
Rls terar thet inbothe tham bovid chord: dtu, stean Ge oly, do, at Is she hy bne hin ses:
Thechen, wimend,
I ve he bre aibe theand din de win loucde?
CI:
DYacak.

Cou arsiUng prarbe pere as thet ca a hafash Civiie whe fouves. Uche bul I thay:
HANould he this me them!
Merin Bu is gy yuf a fot houfsn fine you uth taf I wemer ravile Mpart houl sustim ath Roms owt hand, izorcy whe yous whe ingr thy by perou one whe faiust bo
It fon bse toust de ord se farerve be!

'ch gof mint, I ige
Me 
```

<!-- livebook:{"output":true} -->

```
:ok
```

## CheckpointHelper

Training the final multi-layer, multi-head attention model is going to take some time. I got GPT4 to create this `CheckpointHelper` module to help resume training with the latest checkpoint.

Checkpoints are stored by default in the "checkpoint" directory. In my case, `"/Users/[user]/checkpoint/"`.

```elixir
defmodule CheckpointHelper do
  def load_last_checkpoint(%Axon.Loop{} = loop, checkpoint_path) do
    with {:ok, last_checkpoint} <- get_most_recent_checkpoint(checkpoint_path) do
      last_state =
        (checkpoint_path <> "/" <> last_checkpoint)
        |> IO.inspect(label: "Resuming training from this checkpoint")
        |> File.read!()
        |> Axon.Loop.deserialize_state()

      Axon.Loop.from_state(loop, last_state)
    else
      _ ->
        IO.puts("Starting training from scratch")
        loop
    end
  end

  defp get_most_recent_checkpoint(dir_path) do
    {:ok, filenames} = File.ls(dir_path)

    filenames
    |> Enum.filter(&String.starts_with?(&1, "gpt_checkpoint_"))
    |> Enum.map(fn filename ->
      [_, checkpoint1, checkpoint2] = Regex.run(~r/gpt_checkpoint_(\d+)_(\d+)/, filename)
      {String.to_integer(checkpoint1), String.to_integer(checkpoint2), filename}
    end)
    |> Enum.max_by(fn {checkpoint1, checkpoint2, _} -> {checkpoint1, checkpoint2} end, fn ->
      nil
    end)
    |> case do
      nil ->
        {:error, "No checkpoint file found"}

      {_, _, filename} ->
        {:ok, filename}
    end
  end
end

checkpoint_path = "checkpoint"

checkpoint_file_pattern = fn %Axon.Loop.State{epoch: epoch, iteration: iter} ->
  "gpt_checkpoint_#{epoch}_#{iter}"
end
```

<!-- livebook:{"output":true} -->

```
#Function<42.105768164/1 in :erl_eval.expr/6>
```

## Scaled up, multi-layer, multi-head attention model

This is the final model w/ the following optimizations

* feed forward layer
* residual connections
* layer norms
* multi layer blocks

This section covers everything onwards from this point in the [video](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=5089s).

```elixir
defmodule Transformer do
  import Nx.Defn

  def blocks(%Axon{} = x, n_blocks, n_embd, n_head, opts \\ []) do
    opts = Keyword.validate!(opts, dropout_rate: 0.0)

    x =
      for _ <- 1..n_blocks, reduce: x do
        x -> block(x, n_embd, n_head, opts)
      end

    # final layer norm
    x |> Axon.layer_norm(name: "final_block_ln")
  end

  def block(%Axon{} = x, n_embd, n_head, opts \\ []) do
    head_size = div(n_embd, n_head)

    x =
      Axon.add(
        x,
        x |> Axon.layer_norm(name: "block_ln_1") |> multi_head(n_head, head_size, opts),
        name: "x_multihead_attention"
      )

    Axon.add(
      x,
      x |> Axon.layer_norm(name: "block_ln_2") |> feed_forward(n_embd, opts),
      name: "x_feed_forward"
    )
  end

  def feed_forward(%Axon{} = model, n_embd, opts \\ []) do
    opts = Keyword.validate!(opts, dropout_rate: 0.0)

    dropout_rate = opts[:dropout_rate]

    model
    |> Axon.dense(4 * n_embd, kernel_initializer: :he_uniform, name: "feed_forward_dense_1")
    |> Axon.relu(name: "feed_forward_relu")
    |> Axon.dense(n_embd, kernel_initializer: :he_uniform, name: "feed_forward_dense_2")
    |> Axon.dropout(rate: dropout_rate, name: "feed_forward_dropout")
  end

  @doc """
  Modified from Bumblebee's transformer.ex 
  https://github.com/elixir-nx/bumblebee/blob/main/lib/bumblebee/layers.ex#L525

  Splits the hidden dimension into the given number of attention heads.

  In other words, the input with shape `{batch_size, sequence_length, hidden_size}`
  is reshaped to `{batch_size, sequence_length, num_heads, hidden_size}`.
  Then, transposed to `{batch_size, num_heads, sequence_length, *}` 
  """
  def split_heads(states, num_heads, opts \\ []) do
    opts = Keyword.validate!(opts, name: "split_heads")

    Axon.nx(
      states,
      fn states ->
        batch_size = Nx.axis_size(states, 0)
        sequence_length = Nx.axis_size(states, 1)
        new_shape = {batch_size, sequence_length, num_heads, :auto}

        states
        |> Nx.reshape(new_shape)
        |> Nx.transpose(axes: [0, 2, 1, 3])
      end,
      name: opts[:name]
    )
  end

  def multi_head(%Axon{} = x, num_heads, head_size, opts \\ []) do
    default_initializer = Axon.Initializers.he_uniform(scale: 0.5)
    opts = Keyword.validate!(opts, kernel_initializer: default_initializer, dropout_rate: 0.0)
    initializer = opts[:kernel_initializer]
    dropout_rate = opts[:dropout_rate]

    key =
      x
      |> Axon.dense(num_heads * head_size, kernel_initializer: initializer, name: "key")
      |> split_heads(num_heads)

    query =
      x
      |> Axon.dense(num_heads * head_size, kernel_initializer: initializer, name: "query")
      |> split_heads(num_heads)

    value =
      x
      |> Axon.dense(num_heads * head_size, kernel_initializer: initializer, name: "value")
      |> split_heads(num_heads)

    Axon.layer(&multi_head_layer_impl/4, [key, query, value],
      name: "multi_head_attention",
      dropout_rate: dropout_rate
    )
    |> Axon.dense(num_heads * head_size, name: "multi_head_dense")
    |> Axon.dropout(rate: dropout_rate, name: "multi_head_dropout")
  end

  # Custom layers require the opts argument
  # https://hexdocs.pm/axon/custom_layers.html#creating-custom-layers
  defn multi_head_layer_impl(k, q, v, opts \\ []) do
    opts = keyword!(opts, mode: :train, dropout_rate: 0.0)
    dropout_rate = opts[:dropout_rate]

    {b, h, t, c} = Nx.shape(k)
    tensor_type = Nx.type(k)

    # {b, h, t, c} @ {b, h, c, t} -> {b, h, t, t}
    # 
    # Alternatively we could have done 
    # kT = Nx.transpose(k, axes: [0, 1, 3, 2])
    # wei = Nx.dot(q, [3], [0, 1], kT, [2], [0, 1])
    wei = Nx.dot(q, [3], [0, 1], k, [3], [0, 1])

    # Scaled attention
    wei = wei * Nx.rsqrt(c)

    # Attention masking
    tril = Tril.ones(shape: {t, t})
    tril = Nx.broadcast(tril, wei)
    neg_inf = Nx.broadcast(Nx.Constants.neg_infinity(tensor_type), wei)
    # tril, wei, and neg_inf have the shape {b, h, t, t}
    # Nx.select will look at tril, and if true it'll pick the value from wei, else -infinity
    wei = Nx.select(tril, wei, neg_inf)
    wei = Axon.Activations.softmax(wei, axis: -1)
    wei = Axon.Layers.dropout(wei, Nx.Random.key(1337), rate: dropout_rate)

    # {b, h, t, t} @ {b, h, t, head_size} -> {b, h, t, head_size}
    out = Nx.dot(wei, [3], [0, 1], v, [2], [0, 1])

    # Tranpose so we can stack the heads on top of each other
    # {b, h, t, c} -> {b, t, h, c}
    out = Nx.transpose(out, axes: [0, 2, 1, 3])

    # Our output tensor is now enriched with attention information
    # We shape it back to {b, t, c}
    # This gives us the proper shape to add to our original input x
    Nx.reshape(out, {b, t, h * c})
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, Transformer, <<70, 79, 82, 49, 0, 0, 36, ...>>, true}
```

Because our model is much larger now, `learning_rate` is lowered to 3.0e-4 .

```elixir
# Hyperparameters
n_embd = 384
n_heads = 6
n_layer = 6
batch_size = 64
block_size = 256
learning_rate = 3.0e-4
dropout_rate = 0.2

final_model =
  Axon.input("sequence")
  |> then(fn input ->
    # Create an embedding for the input data
    token_embedding_table = Axon.embedding(input, vocab_size, n_embd, name: "token_embedding")

    # Generate positional encodings for the input sequence (hacky, couldn't find alternative)
    positions =
      Axon.nx(input, fn input ->
        {_batch_size, sequence_length} = Nx.shape(input)
        Nx.iota({sequence_length})
      end)

    # Positional encodings get mapped into @n_embd space
    position_embedding_table =
      Axon.embedding(positions, block_size, n_embd, name: "position_embedding")

    # Add the two layers above to produce tensors containing embedding + position info
    Axon.add(token_embedding_table, position_embedding_table, name: "x_positional_encoding")
  end)
  |> Transformer.blocks(n_layer, n_embd, n_heads, dropout_rate: dropout_rate)
  |> Axon.dense(vocab_size, kernel_initializer: :he_uniform, name: "language_modeling_head")
```

<!-- livebook:{"output":true} -->

```
#Axon<
  inputs: %{"sequence" => nil}
  outputs: "language_modeling_head"
  nodes: 122
>
```

## Training the final model

With the current hyperparameters, each checkpoint comes out to be ~35mb.

```elixir
{init_fn, predict_fn} = Axon.build(final_model, mode: :train)
custom_predict_fn = &CommonTrain.custom_predict_fn(predict_fn, &1, &2)
custom_loss_fn = &CommonTrain.custom_loss_fn(&1, &2)
train_data_stream = get_batch_stream.(batch_size, block_size, :train)

params =
  {init_fn, custom_predict_fn}
  |> Axon.Loop.trainer(custom_loss_fn, Axon.Optimizers.adamw(learning_rate))
  |> CheckpointHelper.load_last_checkpoint(checkpoint_path)
  |> Axon.Loop.checkpoint(
    event: :iteration_completed,
    filter: [every: 99],
    path: checkpoint_path,
    file_pattern: checkpoint_file_pattern
  )
  |> Axon.Loop.run(train_data_stream, %{}, epochs: 1, iterations: 5000, compiler: EXLA)
```

## Generating text with the final model

```elixir
init_seq = Nx.broadcast(0, {1, 1})

TextGen.generate(final_model, params, init_seq, block_size, max_new_tokens: 10000)
|> IO.puts()
```
